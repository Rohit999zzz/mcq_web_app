{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit999zzz/mcq_web_app/blob/main/Multiple_Choice_Question_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB2B7CqgaahC"
      },
      "source": [
        "## Multiple Questions Generation on the given text using NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fwFPS2-aahF"
      },
      "source": [
        "### Problem statement: <br>\n",
        "We are tasked with developing a solution that can automatically generate\n",
        "objective questions with multiple correct answers based on a given chapter from a subject.\n",
        "The generated questions should test the reader's understanding of the chapter and have\n",
        "more than one possible correct answer to increase the complexity and challenge of the\n",
        "questions.The generated questions should not only test the reader's comprehension of the\n",
        "chapter but also encourage them to think beyond the surface level and explore different\n",
        "perspectives and possibilities. Ultimately, the objective of this project is to develop a robust\n",
        "and accurate solution that can aid educators in creating engaging and challenging\n",
        "assessments for their students.\n",
        "\n",
        "### Solution methodology:\n",
        "Here the goal of the problem is a little complex, where developing a solution from scratch takes time and thorough training and fine tuning on certain domain specific data. Although there are many API's available like gpt-3 etc, as this problem is posed in the interest of production usage for any company, we cannot rely on external API's very much. But we have taken small pretrained models here and demonstrated the overall work flow of the project, how to develop the solution for this kind of a problem.\n",
        "In the interesr=t of time, here a model which generates multiple choice questions with a single answer is developed and further steps are given in the future scope of the problem, where more robust model for this particular task will be developed later.\n",
        "\n",
        "### Procedure outline:\n",
        "Very useful resource where the current solution is inspired from : https://www.youtube.com/watch?v=hoCi_bJHyb8\n",
        "1. Given any article we will perform an Extractive summarization ( which means we pick out the important sentences as they are and form a summary ) or Abstractive summarization ( where we get the summary of the text with a slightly changed phrasing or rewriting of the sentences )\n",
        "2. If we perform Extractive summarization, we will perform paraphrasing of sentences using a language model.\n",
        "3. Then we will extract the keywords from the processed text using models like (YAKE, TopicRank, KeyBERT, Multi-partitite algorithm etc )\n",
        "4. Then we will generate the questions about the extracted keywords by giving the processed text along with the keyword to the fine-tuned model.\n",
        "5. Then we generate the distractors/wrong choices for the model using ( wordnet, sense2vec or word2vec etc).\n",
        "6. Now we got the question with the set of correct answer and also the wrong answers, so we will display the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZCrdS7SaahG"
      },
      "source": [
        "#### Importing all the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsY4GYInaahG",
        "outputId": "f2f25ac1-d826-4d6c-e62f-6bc503cfac27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
            "Downloading jellyfish-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.2/347.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jellyfish\n",
            "  Attempting uninstall: jellyfish\n",
            "    Found existing installation: jellyfish 1.1.0\n",
            "    Uninstalling jellyfish-1.1.0:\n",
            "      Successfully uninstalled jellyfish-1.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.38.0 requires jellyfish<1.1.2,>=0.8.9, but you have jellyfish 1.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jellyfish-1.1.3\n",
            "Collecting strsimpy\n",
            "  Downloading strsimpy-0.2.1-py3-none-any.whl.metadata (20 kB)\n",
            "Downloading strsimpy-0.2.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: strsimpy\n",
            "Successfully installed strsimpy-0.2.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "b74be97b8b13495c904071d245630bcd",
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting sense2vec\n",
            "  Downloading sense2vec-2.0.2-py2.py3-none-any.whl.metadata (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from sense2vec) (3.7.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from sense2vec) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from sense2vec) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from sense2vec) (2.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from sense2vec) (1.26.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (8.2.5)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->sense2vec) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->sense2vec) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->sense2vec) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->sense2vec) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->sense2vec) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->sense2vec) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->sense2vec) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->sense2vec) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (0.1.2)\n",
            "Downloading sense2vec-2.0.2-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sense2vec\n",
            "Successfully installed sense2vec-2.0.2\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Collecting textwrap3\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Installing collected packages: textwrap3\n",
            "Successfully installed textwrap3-0.9.2\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pke (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pke\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9298 sha256=35fe35b3c66e59cf436c5432a2d6d42ae9142da7d7c70714a9b61756e2bf202f\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/20/47/f03dfa8a7239c54cbc44ff7389eefbf888d2c1873edaaec888\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext\n",
            "Successfully installed flashtext-2.7\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting similarity\n",
            "  Downloading similarity-0.0.1-py3-none-any.whl.metadata (677 bytes)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from similarity) (0.8.1)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.11/dist-packages (from similarity) (1.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from similarity) (1.26.4)\n",
            "Collecting interaction (from similarity)\n",
            "  Downloading interaction-1.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Downloading similarity-0.0.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading interaction-1.3-py3-none-any.whl (7.2 kB)\n",
            "Installing collected packages: interaction, similarity\n",
            "Successfully installed interaction-1.3 similarity-0.0.1\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.11/dist-packages (1.1.3)\n",
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-xtle0mg6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-xtle0mg6\n",
            "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (3.9.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.6.1)\n",
            "Collecting unidecode (from pke==2.0.0)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.4.2)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->pke==2.0.0) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->pke==2.0.0) (2024.11.6)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pke==2.0.0) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (0.1.2)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pke\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160627 sha256=94f17208263fc98e3ffb094c2b94b7b63a85e8f745a1e2e1511a2590ebbfafa2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i3s9gq1d/wheels/b2/b9/39/32b734e95546f83e8d2584c49db5bf486b1ac093a94aef6f33\n",
            "Successfully built pke\n",
            "Installing collected packages: unidecode, pke\n",
            "Successfully installed pke-2.0.0 unidecode-1.3.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade jellyfish # This line ensures the jellyfish package is updated\n",
        "!pip install strsimpy # Install the strsimpy package\n",
        "\n",
        "#import all the neccessary libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install sense2vec\n",
        "!pip install sentence-transformers\n",
        "!pip install textwrap3\n",
        "!pip install numpy\n",
        "!pip install nltk\n",
        "!pip install pke\n",
        "!pip install flashtext\n",
        "!pip install sklearn\n",
        "!pip install similarity\n",
        "# Update jellyfish package\n",
        "!pip install --upgrade jellyfish # This line is added to update jellyfish\n",
        "\n",
        "# Installing pke package using pip\n",
        "!pip install git+https://github.com/boudinfl/pke.git  # Install pke from GitHub\n",
        "from sense2vec import Sense2Vec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# The user likely intended to use the built-in textwrap module\n",
        "import textwrap as wrap # Changed the import statement\n",
        "import random\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pke # Now you should be able to import pke\n",
        "import traceback\n",
        "from flashtext import KeywordProcessor\n",
        "from collections import OrderedDict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('omw-1.4')\n",
        "# Instead of importing directly from similarity\n",
        "# We'll use strsimpy's JaroWinkler for similar functionality\n",
        "from strsimpy.jaro_winkler import JaroWinkler # Import JaroWinkler from strsimpy\n",
        "\n",
        "# Create a NormalizedLevenshtein class\n",
        "# that behaves similar to the original import\n",
        "class NormalizedLevenshtein:\n",
        "    def __init__(self):\n",
        "        self.jarowinkler = JaroWinkler()\n",
        "\n",
        "    def similarity(self, s1, s2):\n",
        "        return self.jarowinkler.similarity(s1, s2)\n",
        "\n",
        "import pickle\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1TbkSR-jiT5",
        "outputId": "8ca7b2e2-fcf3-4fee-c356-8740c7ef5be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install --upgrade nltk # This will update NLTK to the latest version, just in case an older version was causing issues\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab') # This line should download the required 'punkt_tab' data for NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBArINvkaahI"
      },
      "source": [
        "#### Checking for GPU availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RALGB5KnaahI",
        "outputId": "db1a5f20-191f-4bf2-b767-f0ee5a91685a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BJlfGOWaahI"
      },
      "source": [
        "#### Getting the pretrained models and tokenizers from disc:\n",
        "1. Sense2vec model trained on reddit 2019 data.\n",
        "2. T5-base pretrained summarizer model.\n",
        "3. Pretrained T5 question generator model trained on Squad-V1 dataset.\n",
        "4. Msmarco-distilbert-base-v2 sentence transformer pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE66wZbEaahJ",
        "outputId": "e4dccf15-3486-49c2-d0fb-135947848b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sense2vec in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from sense2vec) (3.7.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from sense2vec) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from sense2vec) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from sense2vec) (2.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from sense2vec) (1.26.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (8.2.5)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->sense2vec) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->sense2vec) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->sense2vec) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->sense2vec) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->sense2vec) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->sense2vec) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->sense2vec) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->sense2vec) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (0.1.2)\n",
            "An error occurred while loading the Sense2Vec model: Can't read file: /root/.sense2vec/reddit_vectors-sm/strings.json\n",
            "Summary model found in the disc, model is loaded successfully.\n",
            "Summary tokenizer found in the disc and is loaded successfully.\n",
            "Question model found in the disc, model is loaded successfully.\n",
            "Question tokenizer found in the disc, model is loaded successfully.\n",
            "Sentence transformer model found in the disc, model is loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "#we need to download the 2015 trained on reddit sense2vec model as it is shown to give better results than the 2019 one.python -m sense2vec.download reddit_vectors-sm # Downloading sense2vec model\n",
        "# ... (Previous code) ...\n",
        "\n",
        "# # ... (Previous code) ...\n",
        "\n",
        "# Ensure sense2vec is installed\n",
        "!pip install sense2vec\n",
        "\n",
        "# Import sense2vec\n",
        "from sense2vec import Sense2Vec\n",
        "import os\n",
        "\n",
        "# Define a function to load or download the model,\n",
        "# and handle potential errors:\n",
        "def load_sense2vec_model(s2v_path):\n",
        "    try:\n",
        "        s2v = Sense2Vec().from_disk(s2v_path)\n",
        "        print(\"Sense2Vec model loaded successfully!\")\n",
        "        return s2v\n",
        "    except (FileNotFoundError, OSError):  # Handle both FileNotFoundError and OSError\n",
        "        print(f\"Model not found or corrupted at {s2v_path}. Redownloading...\")\n",
        "        import sys\n",
        "        # Download to user's home directory:\n",
        "        download_dir = os.path.join(os.path.expanduser(\"~\"), \".sense2vec\")\n",
        "        os.makedirs(download_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "        s2v_path = os.path.join(download_dir, \"reddit_vectors-sm\")\n",
        "        # Explicitly set the download path:\n",
        "        !python -m sense2vec.download --output_dir {download_dir} reddit_vectors-sm\n",
        "        s2v = Sense2Vec().from_disk(s2v_path)\n",
        "        print(\"Sense2Vec model redownloaded and loaded successfully!\")\n",
        "        return s2v\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the Sense2Vec model: {e}\")\n",
        "        return None\n",
        "\n",
        "# Get the path where sense2vec models are downloaded (user's home directory):\n",
        "s2v_path = os.path.join(os.path.expanduser(\"~\"), \".sense2vec\", \"reddit_vectors-sm\")\n",
        "\n",
        "# Load or download the model using the function:\n",
        "s2v = load_sense2vec_model(s2v_path)\n",
        "\n",
        "# ... (Rest of your code) ...\n",
        "# ... Inside your get_distractors\n",
        "\n",
        "#getitng the summary model and its tokenizer\n",
        "if os.path.exists(\"t5_summary_model.pkl\"):\n",
        "    with open('t5_summary_model.pkl', 'rb') as f:\n",
        "        summary_model = pickle.load(f)\n",
        "    print(\"Summary model found in the disc, model is loaded successfully.\")\n",
        "\n",
        "else:\n",
        "    print(\"Summary model does not exists in the path specified, downloading the model from web....\")\n",
        "    start_time = time.time()\n",
        "    summary_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(\"downloaded the summary model in \",(end_time-start_time)/60,\" min , now saving it to disc...\")\n",
        "\n",
        "    with open(\"t5_summary_model.pkl\", 'wb') as f:\n",
        "        pickle.dump(summary_model,f)\n",
        "\n",
        "    print(\"Done. Saved the model to disc.\")\n",
        "\n",
        "if os.path.exists(\"t5_summary_tokenizer.pkl\"):\n",
        "    with open('t5_summary_tokenizer.pkl', 'rb') as f:\n",
        "        summary_tokenizer = pickle.load(f)\n",
        "    print(\"Summary tokenizer found in the disc and is loaded successfully.\")\n",
        "else:\n",
        "    print(\"Summary tokenizer does not exists in the path specified, downloading the model from web....\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(\"downloaded the summary tokenizer in \",(end_time-start_time)/60,\" min , now saving it to disc...\")\n",
        "\n",
        "    with open(\"t5_summary_tokenizer.pkl\",'wb') as f:\n",
        "        pickle.dump(summary_tokenizer,f)\n",
        "\n",
        "    print(\"Done. Saved the tokenizer to disc.\")\n",
        "\n",
        "\n",
        "#Getting question model and tokenizer\n",
        "if os.path.exists(\"t5_question_model.pkl\"):\n",
        "    with open('t5_question_model.pkl', 'rb') as f:\n",
        "        question_model = pickle.load(f)\n",
        "    print(\"Question model found in the disc, model is loaded successfully.\")\n",
        "else:\n",
        "    print(\"Question model does not exists in the path specified, downloading the model from web....\")\n",
        "    start_time= time.time()\n",
        "    question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(\"downloaded the question model in \",(end_time-start_time)/60,\" min , now saving it to disc...\")\n",
        "\n",
        "    with open(\"t5_question_model.pkl\", 'wb') as f:\n",
        "        pickle.dump(question_model,f)\n",
        "\n",
        "    print(\"Done. Saved the model to disc.\")\n",
        "\n",
        "if os.path.exists(\"t5_question_tokenizer.pkl\"):\n",
        "    with open('t5_question_tokenizer.pkl', 'rb') as f:\n",
        "        question_tokenizer = pickle.load(f)\n",
        "    print(\"Question tokenizer found in the disc, model is loaded successfully.\")\n",
        "else:\n",
        "    print(\"Question tokenizer does not exists in the path specified, downloading the model from web....\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "    end_time=time.time()\n",
        "\n",
        "    print(\"downloaded the question tokenizer in \",(end_time-start_time)/60,\" min , now saving it to disc...\")\n",
        "\n",
        "    with open(\"t5_question_tokenizer.pkl\",'wb') as f:\n",
        "        pickle.dump(question_tokenizer,f)\n",
        "\n",
        "    print(\"Done. Saved the tokenizer to disc.\")\n",
        "\n",
        "#Loading the models in to GPU if available\n",
        "summary_model = summary_model.to(device)\n",
        "question_model = question_model.to(device)\n",
        "\n",
        "#Getting the sentence transformer model and its tokenizer\n",
        "# paraphrase-distilroberta-base-v1\n",
        "if os.path.exists(\"sentence_transformer_model.pkl\"):\n",
        "    with open(\"sentence_transformer_model.pkl\",'rb') as f:\n",
        "        sentence_transformer_model = pickle.load(f)\n",
        "    print(\"Sentence transformer model found in the disc, model is loaded successfully.\")\n",
        "else:\n",
        "    print(\"Sentence transformer model does not exists in the path specified, downloading the model from web....\")\n",
        "    start_time=time.time()\n",
        "    sentence_transformer_model = SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-v2\")\n",
        "    end_time=time.time()\n",
        "\n",
        "    print(\"downloaded the sentence transformer in \",(end_time-start_time)/60,\" min , now saving it to disc...\")\n",
        "\n",
        "    with open(\"sentence_transformer_model.pkl\",'wb') as f:\n",
        "        pickle.dump(sentence_transformer_model,f)\n",
        "\n",
        "    print(\"Done saving to disc.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-vntDe_aahJ"
      },
      "source": [
        "#### Defining all the utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oimOQbGCaahJ"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def postprocesstext (content):\n",
        "  \"\"\"\n",
        "  this function takes a piece of text (content), tokenizes it into sentences, capitalizes the first letter of each sentence, and then concatenates the processed sentences into a single string, which is returned as the final result. The purpose of this function could be to format the input content by ensuring that each sentence starts with an uppercase letter.\n",
        "  \"\"\"\n",
        "  final=\"\"\n",
        "  for sent in sent_tokenize(content):\n",
        "    sent = sent.capitalize()\n",
        "    final = final +\" \"+sent\n",
        "  return final\n",
        "\n",
        "def summarizer(text,model,tokenizer):\n",
        "  \"\"\"\n",
        "  This function takes the given text along with the model and tokenizer, which summarize the large text into useful information\n",
        "  \"\"\"\n",
        "  text = text.strip().replace(\"\\n\",\" \")\n",
        "  text = \"summarize: \"+text\n",
        "  # print (text)\n",
        "  max_len = 512\n",
        "  encoding = tokenizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=3,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  min_length = 75,\n",
        "                                  max_length=300)\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "  summary = dec[0]\n",
        "  summary = postprocesstext(summary)\n",
        "  summary= summary.strip()\n",
        "\n",
        "  return summary\n",
        "\n",
        "def get_nouns_multipartite(content):\n",
        "    \"\"\"\n",
        "    This function takes the content text given and then outputs the phrases which are build around the nouns , so that we can use them for context based distractors\n",
        "    \"\"\"\n",
        "    out=[]\n",
        "    try:\n",
        "        extractor = pke.unsupervised.MultipartiteRank()\n",
        "        extractor.load_document(input=content,language='en')\n",
        "        #    not contain punctuation marks or stopwords as candidates.\n",
        "        #pos = {'PROPN','NOUN',}\n",
        "        pos = {'PROPN', 'NOUN', 'ADJ', 'VERB', 'ADP', 'ADV', 'DET', 'CONJ', 'NUM', 'PRON', 'X'}\n",
        "\n",
        "        #pos = {'PROPN','NOUN'}\n",
        "        stoplist = list(string.punctuation)\n",
        "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
        "        stoplist += stopwords.words('english')\n",
        "        # extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
        "        extractor.candidate_selection( pos=pos)\n",
        "        # 4. build the Multipartite graph and rank candidates using random walk,\n",
        "        #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
        "        #    threshold/method parameters.\n",
        "        extractor.candidate_weighting(alpha=1.1,\n",
        "                                      threshold=0.75,\n",
        "                                      method='average')\n",
        "        keyphrases = extractor.get_n_best(n=15)\n",
        "\n",
        "\n",
        "        for val in keyphrases:\n",
        "            out.append(val[0])\n",
        "    except:\n",
        "        out = []\n",
        "        #traceback.print_exc()\n",
        "\n",
        "    return out\n",
        "\n",
        "def get_keywords(originaltext):\n",
        "  \"\"\"\n",
        "  This function takes the original text and the summary text and generates keywords from both which ever are more relevant\n",
        "  This is done by checking the keywords generated from the original text to those generated from the summary, so that we get important ones\n",
        "  \"\"\"\n",
        "  keywords = get_nouns_multipartite(originaltext)\n",
        "  #print (\"keywords unsummarized: \",keywords)\n",
        "  #keyword_processor = KeywordProcessor()\n",
        "  #for keyword in keywords:\n",
        "    #keyword_processor.add_keyword(keyword)\n",
        "\n",
        "  #keywords_found = keyword_processor.extract_keywords(summarytext)\n",
        "  #keywords_found = list(set(keywords_found))\n",
        "  #print (\"keywords_found in summarized: \",keywords_found)\n",
        "\n",
        "  #important_keywords =[]\n",
        "  #for keyword in keywords:\n",
        "    #if keyword in keywords_found:\n",
        "      #important_keywords.append(keyword)\n",
        "\n",
        "  #return important_keywords\n",
        "  return keywords\n",
        "\n",
        "def get_question(context,answer,model,tokenizer):\n",
        "  \"\"\"\n",
        "  This function takes the input context text, pretrained model along with the tokenizer and the keyword and the answer and then generates the question from the large paragraph\n",
        "  \"\"\"\n",
        "  text = \"context: {} answer: {}\".format(context,answer)\n",
        "  encoding = tokenizer.encode_plus(text,max_length=384, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=72)\n",
        "\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "\n",
        "\n",
        "  Question = dec[0].replace(\"question:\",\"\")\n",
        "  Question= Question.strip()\n",
        "  return Question\n",
        "\n",
        "def filter_same_sense_words(original,wordlist):\n",
        "\n",
        "  \"\"\"\n",
        "  This is used to filter the words which are of same sense, where it takes the wordlist which has the sense of the word attached as the string along with the word itself.\n",
        "  \"\"\"\n",
        "  filtered_words=[]\n",
        "  base_sense =original.split('|')[1]\n",
        "  #print (base_sense)\n",
        "  for eachword in wordlist:\n",
        "    if eachword[0].split('|')[1] == base_sense:\n",
        "      filtered_words.append(eachword[0].split('|')[0].replace(\"_\", \" \").title().strip())\n",
        "  return filtered_words\n",
        "\n",
        "def get_highest_similarity_score(wordlist,wrd):\n",
        "  \"\"\"\n",
        "  This function takes the given word along with the wordlist and then gives out the max-score which is the levenshtein distance for the wrong answers\n",
        "  because we need the options which are very different from one another but relating to the same context.\n",
        "  \"\"\"\n",
        "  score=[]\n",
        "  normalized_levenshtein = NormalizedLevenshtein()\n",
        "  for each in wordlist:\n",
        "    score.append(normalized_levenshtein.similarity(each.lower(),wrd.lower()))\n",
        "  return max(score)\n",
        "\n",
        "def sense2vec_get_words(word,s2v,topn,question):\n",
        "    \"\"\"\n",
        "    This function takes the input word, sentence to vector model and top similar words and also the question\n",
        "    Then it computes the sense of the given word\n",
        "    then it gets the words which are of same sense but are most similar to the given word\n",
        "    after that we we return the list of words which satisfy the above mentioned criteria\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    #print (\"word \",word)\n",
        "    try:\n",
        "      sense = s2v.get_best_sense(word, senses= [\"NOUN\", \"PERSON\",\"PRODUCT\",\"LOC\",\"ORG\",\"EVENT\",\"NORP\",\"WORK OF ART\",\"FAC\",\"GPE\",\"NUM\",\"FACILITY\"])\n",
        "      most_similar = s2v.most_similar(sense, n=topn)\n",
        "      # print (most_similar)\n",
        "      output = filter_same_sense_words(sense,most_similar)\n",
        "      #print (\"Similar \",output)\n",
        "    except:\n",
        "      output =[]\n",
        "\n",
        "    threshold = 0.6\n",
        "    final=[word]\n",
        "    checklist =question.split()\n",
        "    for x in output:\n",
        "      if get_highest_similarity_score(final,x)<threshold and x not in final and x not in checklist:\n",
        "        final.append(x)\n",
        "\n",
        "    return final[1:]\n",
        "\n",
        "def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
        "    \"\"\"\n",
        "    The mmr function takes document and word embeddings, along with other parameters, and uses the Maximal Marginal Relevance (MMR) algorithm to extract a specified number of keywords/keyphrases from the document. The MMR algorithm balances the relevance of keywords with their diversity, helping to select keywords that are both informative and distinct from each other.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract similarity within words, and between words and the document\n",
        "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
        "    word_similarity = cosine_similarity(word_embeddings)\n",
        "\n",
        "    # Initialize candidates and already choose best keyword/keyphrase\n",
        "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "\n",
        "    for _ in range(top_n - 1):\n",
        "        # Extract similarities within candidates and\n",
        "        # between candidates and selected keywords/phrases\n",
        "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
        "\n",
        "        # Calculate MMR\n",
        "        mmr = (lambda_param) * candidate_similarities - (1-lambda_param) * target_similarities.reshape(-1, 1)\n",
        "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "        # Update keywords & candidates\n",
        "        keywords_idx.append(mmr_idx)\n",
        "        candidates_idx.remove(mmr_idx)\n",
        "\n",
        "    return [words[idx] for idx in keywords_idx]\n",
        "\n",
        "def get_distractors_wordnet(word):\n",
        "    \"\"\"\n",
        "    the get_distractors_wordnet function uses WordNet to find a relevant synset for the input word and then generates distractor words by looking at hyponyms of the hypernym associated with the input word. These distractors are alternative words related to the input word and can be used, for example, in educational or language-related applications to provide choices for a given word.\n",
        "    \"\"\"\n",
        "    distractors=[]\n",
        "    try:\n",
        "      syn = wn.synsets(word,'n')[0]\n",
        "\n",
        "      word= word.lower()\n",
        "      orig_word = word\n",
        "      if len(word.split())>0:\n",
        "          word = word.replace(\" \",\"_\")\n",
        "      hypernym = syn.hypernyms()\n",
        "      if len(hypernym) == 0:\n",
        "          return distractors\n",
        "      for item in hypernym[0].hyponyms():\n",
        "          name = item.lemmas()[0].name()\n",
        "          #print (\"name \",name, \" word\",orig_word)\n",
        "          if name == orig_word:\n",
        "              continue\n",
        "          name = name.replace(\"_\",\" \")\n",
        "          name = \" \".join(w.capitalize() for w in name.split())\n",
        "          if name is not None and name not in distractors:\n",
        "              distractors.append(name)\n",
        "    except:\n",
        "      print (\"Wordnet distractors not found\")\n",
        "    return distractors\n",
        "\n",
        "def get_distractors (word,origsentence,sense2vecmodel,sentencemodel,top_n,lambdaval):\n",
        "  \"\"\"\n",
        "  this function generates distractor words (answer choices) for a given target word in the context of a provided sentence. It selects distractors based on their similarity to the target word's context and ensures that the target word itself is not included among the distractors. This function is useful for creating multiple-choice questions or answer options in natural language processing tasks.\n",
        "  \"\"\"\n",
        "  distractors = sense2vec_get_words(word,sense2vecmodel,top_n,origsentence)\n",
        "  #print (\"distractors \",distractors)\n",
        "  if len(distractors) ==0:\n",
        "    return distractors\n",
        "  distractors_new = [word.capitalize()]\n",
        "  distractors_new.extend(distractors)\n",
        "  # print (\"distractors_new .. \",distractors_new)\n",
        "\n",
        "  embedding_sentence = origsentence+ \" \"+word.capitalize()\n",
        "  # embedding_sentence = word\n",
        "  keyword_embedding = sentencemodel.encode([embedding_sentence])\n",
        "  distractor_embeddings = sentencemodel.encode(distractors_new)\n",
        "\n",
        "  # filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors,4,0.7)\n",
        "  max_keywords = min(len(distractors_new),5)\n",
        "  filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors_new,max_keywords,lambdaval)\n",
        "  # filtered_keywords = filtered_keywords[1:]\n",
        "  final = [word.capitalize()]\n",
        "  for wrd in filtered_keywords:\n",
        "    if wrd.lower() !=word.lower():\n",
        "      final.append(wrd.capitalize())\n",
        "  final = final[1:]\n",
        "  return final\n",
        "def get_mca_questions(context: str):\n",
        "    \"\"\"\n",
        "    This function generates multiple-choice questions based on a given context. It summarizes the context,\n",
        "    extracts important keywords, generates questions related to those keywords, and provides randomized\n",
        "    answer choices, including the correct answer, for each question.\n",
        "    \"\"\"\n",
        "    # Summarize the context text\n",
        "    summarized_text = summarizer(context, summary_model, summary_tokenizer)  # Summarization of the context\n",
        "\n",
        "    # Extract keywords or important phrases from the summarized text\n",
        "    imp_keywords = get_keywords(summarized_text)  # You should extract keywords from the summarized text, not the original text\n",
        "\n",
        "    output_list = []\n",
        "\n",
        "    for keyword in imp_keywords:\n",
        "        output = \"\"\n",
        "\n",
        "        # Generate a question based on the keyword and the summarized text\n",
        "        ques = get_question(summarized_text, keyword, question_model, question_tokenizer)  # Generate the question\n",
        "\n",
        "        # Generate distractors for the question\n",
        "        distractors = get_distractors(keyword.capitalize(), ques, s2v, sentence_transformer_model, 40, 0.2)\n",
        "\n",
        "        output += ques + \"\\n\"  # Add the question text\n",
        "\n",
        "        if len(distractors) == 0:\n",
        "            distractors = imp_keywords  # Use the keywords themselves if no distractors are found\n",
        "\n",
        "        # Randomly choose where to place the correct answer\n",
        "        random_integer = random.randint(0, 3)  # Randomize answer order\n",
        "        alpha_list = ['(a)', '(b)', '(c)', '(d)']\n",
        "\n",
        "        # Add the options (correct answer and distractors)\n",
        "        for d, distractor in enumerate(distractors[:4]):\n",
        "            if d == random_integer:\n",
        "                output += alpha_list[d] + keyword + \"\\n\"  # Correct answer in random position\n",
        "            else:\n",
        "                output += alpha_list[d] + distractor + \"\\n\"  # Distractor options\n",
        "\n",
        "        # Add the correct answer to the output\n",
        "        output += \"Correct answer is: \" + alpha_list[random_integer] + \"\\n\\n\"\n",
        "\n",
        "        output_list.append(output)  # Add this question block to the output list\n",
        "\n",
        "    return output_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpOw60F4aahK"
      },
      "source": [
        "#### Testing the solution on an example text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bhZaZDVaahK"
      },
      "source": [
        "Example: 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSyyiladaahK",
        "outputId": "94296be1-c865-46d8-861f-aaca6c4fb562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What has led to the evolution of Lamborghini?\n",
            "(a)groundbreaking innovations\n",
            "(b)economic downturns\n",
            "(c)malaysian investment group mycom setdco\n",
            "(d)groundbreaking innovations\n",
            "Correct answer is: (d)\n",
            "\n",
            "\n",
            "Along with ownership changes, innovations and groundbreaking innovations, what has influenced the evolution of Lamborghini?\n",
            "(a)groundbreaking innovations\n",
            "(b)economic downturns\n",
            "(c)malaysian investment group mycom setdco\n",
            "(d)suvs\n",
            "Correct answer is: (b)\n",
            "\n",
            "\n",
            "What investment group acquired Lamborghini in 1994?\n",
            "(a)groundbreaking innovations\n",
            "(b)economic downturns\n",
            "(c)malaysian investment group mycom setdco\n",
            "(d)suvs\n",
            "Correct answer is: (c)\n",
            "\n",
            "\n",
            "Lamborghini manufactures luxury sports cars and what else?\n",
            "(a)groundbreaking innovations\n",
            "(b)economic downturns\n",
            "(c)suvs\n",
            "(d)suvs\n",
            "Correct answer is: (c)\n",
            "\n",
            "\n",
            "What is the name of the illustrious italian manufacturer of luxury sports cars?\n",
            "(a)groundbreaking innovations\n",
            "(b)economic downturns\n",
            "(c)malaysian investment group mycom setdco\n",
            "(d)automobili lamborghini\n",
            "Correct answer is: (d)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text_1 = \"Automobili Lamborghini, the illustrious Italian manufacturer of luxury sports cars and SUVs, is headquartered in the picturesque Sant'Agata Bolognese. This renowned automotive institution boasts a storied legacy, and its contemporary success is firmly underpinned by a fascinating history that has seen it evolve through ownership changes, economic downturns, and groundbreaking innovations.\\\n",
        "Ferruccio Lamborghini, a prominent Italian industrialist with a passion for automobiles, laid the foundation for this iconic marque in 1963. His vision was audacious - to challenge the supremacy of Ferrari, the undisputed titan of Italian sports cars. Under Ferruccio's guidance, Automobili Ferruccio Lamborghini S.p.A. was established, and it immediately began making waves in the automotive world.\\\n",
        "One of the hallmarks of Lamborghini's early years was its distinctive rear mid-engine, rear-wheel-drive layout. This design philosophy became synonymous with Lamborghini's commitment to creating high-performance vehicles. The company's inaugural models, such as the 350 GT, arrived in the mid-1960s and showcased Lamborghini's dedication to precision engineering and uncompromising quality.\\\n",
        "Lamborghini's ascendancy was nothing short of meteoric during its formative decade. It consistently pushed the boundaries of automotive technology and design. However, the heady days of growth were met with a sudden downturn when the world faced the harsh realities of the 1973 global financial crisis and the subsequent oil embargo. Lamborghini, like many other automakers, grappled with plummeting sales and financial instability.\\\n",
        "Ownership of Lamborghini underwent multiple transitions in the wake of these challenges. The company faced bankruptcy in 1978, marking a turbulent chapter in its history. The ownership baton changed hands several times, with different entities attempting to steer the storied brand to calmer waters.\\\n",
        "In 1987, American automaker Chrysler Corporation took the helm at Lamborghini. The Chrysler era saw Lamborghini continue to produce remarkable vehicles like the Diablo while operating under the umbrella of a global conglomerate. However, it was not a permanent arrangement.\\\n",
        "In 1994, Malaysian investment group Mycom Setdco and Indonesian group V'Power Corporation acquired Lamborghini, signaling another phase of transformation for the company. These new custodians brought fresh perspectives and investment to the brand, fueling its resurgence.\\\n",
        "A significant turning point occurred in 1998 when Mycom Setdco and V'Power sold Lamborghini to the Volkswagen Group, which placed the Italian marque under the stewardship of its Audi division. This move brought newfound stability and resources, ensuring Lamborghini's enduring presence in the luxury sports car arena.\\\n",
        "Over the ensuing years, Lamborghini witnessed remarkable expansions in its product portfolio. The V10-powered Huracán captured the hearts of sports car enthusiasts with its exquisite design and formidable performance. Simultaneously, Lamborghini ventured into the SUV market with the Urus, a groundbreaking vehicle powered by a potent twin-turbo V8 engine. This diversification allowed Lamborghini to cater to a broader range of customers without compromising on its commitment to luxury and performance.\\\n",
        "While these successes were noteworthy, Lamborghini was not immune to the challenges posed by global economic fluctuations. In the late 2000s, during the worldwide financial crisis and the subsequent economic downturn, Lamborghini's sales experienced a significant decline, illustrating the brand's vulnerability to external economic factors.\\\n",
        "Despite these challenges, Lamborghini maintained its relentless pursuit of automotive excellence. The company's flagship model, the V12-powered Aventador, reached the pinnacle of automotive engineering and design before concluding its production run in 2022. However, the story does not end here. Lamborghini is set to introduce the Revuelto, a V12/electric hybrid model, in 2024, exemplifying its commitment to embracing cutting-edge technologies and pushing the boundaries of performance.\\\n",
        "In addition to its road car production, Lamborghini has made notable contributions to other industries. The company manufactures potent V12 engines for offshore powerboat racing, further underscoring its prowess in high-performance engineering.\\\n",
        "Interestingly, Lamborghini's legacy extends beyond the realm of automobiles. Ferruccio Lamborghini founded Lamborghini Trattori in 1948, a separate entity from the automobile manufacturer, which continues to produce tractors to this day.\\\n",
        "Lamborghini's rich history is also intertwined with the world of motorsport. In a stark contrast to his rival Enzo Ferrari, Ferruccio Lamborghini decided early on not to engage in factory-supported racing, considering it too expensive and resource-intensive. Nonetheless, Lamborghini's engineers, many of whom were passionate about racing, embarked on ambitious projects, including the development of the iconic Miura sports coupe, which possessed racing potential while being road-friendly. This project marked a pivotal moment in Lamborghini's history, showcasing its ability to create vehicles that could excel on both the track and the road.Despite Ferruccio's reluctance, Lamborghini did make some forays into motorsport. In the mid-1970s, while under the management of Georges-Henri Rossetti, Lamborghini collaborated with BMW to develop and manufacture 400 cars for BMW, a venture intended to meet Group 4 homologation requirements. However, due to financial instability and delays in development, BMW eventually took control of the project, finishing it without Lamborghini's involvement.\\\n",
        "Lamborghini also briefly supplied engines to Formula One teams from 1989 to 1993. Teams like Larrousse, Lotus, Ligier, Minardi, and Modena utilized Lamborghini power units during this period. Lamborghini's best result in Formula One was achieved when Aguri Suzuki finished third at the 1990 Japanese Grand Prix.\\\n",
        "In addition to Formula One, Lamborghini was involved in other racing series. Notably, racing versions of the Diablo were developed for the Diablo Supertrophy, a single-model racing series that ran from 1996 to 1999. The Murciélago R-GT, a production racing car, was created to compete in events like the FIA GT Championship and the American Le Mans Series in 2004, achieving notable results in its racing endeavors.\\\n",
        "Lamborghini's connection with motorsport reflects the brand's commitment to engineering excellence, even though it shied away from factory-backed racing for much of its history.\\\n",
        "Beyond the realms of automotive engineering, Lamborghini has carved a distinct niche in the world of branding. The company licenses its prestigious brand to manufacturers who produce a wide array of Lamborghini-branded consumer goods, including scale models, clothing, accessories, bags, electronics, and even laptop computers. This strategic approach has enabled Lamborghini to extend its brand reach beyond the confines of the automotive industry.\\\n",
        "One fascinating aspect of Lamborghini's identity is its deep connection with the world of bullfighting. In 1962, Ferruccio Lamborghini visited the ranch of Don Eduardo Miura, a renowned breeder of Spanish fighting bulls. Impressed by the majestic Miura animals, Ferruccio decided to adopt a raging bull as the emblem for his burgeoning automaker. This emblem, now iconic, symbolizes Lamborghini's passion for performance, power, and the thrill of the chase.\\\n",
        "Lamborghini's vehicle nomenclature also reflects this bullfighting heritage, with many models bearing the names of famous fighting bulls or bull-related themes. The Miura, named after the Miura bulls, set the precedent, and subsequent models like the Murciélago, Gallardo, and Aventador continued this tradition.\\\n",
        "Furthermore, Lamborghini has enthusiastically embraced emerging automotive technologies, responding to environmental concerns and changing consumer preferences. The Sian, introduced as the company's first hybrid model, showcases Lamborghini's commitment to sustainable performance. With its innovative hybrid powertrain, the Sian combines electric propulsion with a naturally aspirated V12 engine to deliver breathtaking performance while minimizing emissions.\\\n",
        "Looking ahead, Lamborghini has ambitious plans to produce an all-electric vehicle, aligning with the broader industry trend towards electrification. While traditionalists may lament the absence of roaring V12 engines, Lamborghini recognizes the importance of evolving with the times, ensuring that future generations of enthusiasts can experience the thrill of a Lamborghini while contributing to a more sustainable future.\\\n",
        "In summary, Automobili Lamborghini stands as a testament to the enduring allure of Italian craftsmanship and automotive excellence. From its audacious beginnings as a challenger to Ferrari, Lamborghini has weathered storms, embraced innovation, and left an indelible mark on the world of sports cars. Its legacy is one of design brilliance, relentless pursuit of power, and a commitment to pushing the boundaries of what's possible in the realm of high-performance automobiles. Whether through its iconic V12-powered supercars, groundbreaking hybrids, or electrifying visions of the future, Lamborghini continues to captivate the hearts of automotive enthusiasts worldwide, cementing its status as a legendary and iconic brand.\"\n",
        "\n",
        "final_questions = get_mca_questions(text_1)\n",
        "for q in final_questions:\n",
        "    print(q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJXiWLHJaahL"
      },
      "source": [
        "Example: 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH3Si9fYaahL",
        "outputId": "0c6ae094-3140-49d3-a3a5-9864c8dfffe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the name of Musk's venture?\n",
            "(a)elon musk\n",
            "(b)bitcoin\n",
            "(c)musk\n",
            "(d)elon musk\n",
            "Correct answer is : (d)\n",
            "\n",
            "\n",
            "The narrative explores the multifaceted relationship between elon musk and what?\n",
            "(a)bitcoin\n",
            "(b)bitcoin\n",
            "(c)musk\n",
            "(d)tesla\n",
            "Correct answer is : (a)\n",
            "\n",
            "\n",
            "What is the name of elon musk?\n",
            "(a)elon musk\n",
            "(b)musk\n",
            "(c)musk\n",
            "(d)tesla\n",
            "Correct answer is : (b)\n",
            "\n",
            "\n",
            "Along with spacex, neuralink and boring company, what is a notable venture of Elon Musk?\n",
            "(a)tesla\n",
            "(b)bitcoin\n",
            "(c)musk\n",
            "(d)tesla\n",
            "Correct answer is : (a)\n",
            "\n",
            "\n",
            "What is elon musk known for?\n",
            "(a)tweets\n",
            "(b)bitcoin\n",
            "(c)musk\n",
            "(d)tesla\n",
            "Correct answer is : (a)\n",
            "\n",
            "\n",
            "What aspect of bitcoin is elon musk known for?\n",
            "(a)price\n",
            "(b)bitcoin\n",
            "(c)musk\n",
            "(d)tesla\n",
            "Correct answer is : (a)\n",
            "\n",
            "\n",
            "The narrative explores the relationship between elon musk and what?\n",
            "(a)bitcoin investment\n",
            "(b)bitcoin\n",
            "(c)musk\n",
            "(d)tesla\n",
            "Correct answer is : (a)\n",
            "\n",
            "\n",
            "Along with tesla and neuralink, what is a notable venture of Elon Musk?\n",
            "(a)elon musk\n",
            "(b)spacex\n",
            "(c)musk\n",
            "(d)tesla\n",
            "Correct answer is : (b)\n",
            "\n",
            "\n",
            "What has elon musk been known for?\n",
            "(a)elon musk\n",
            "(b)volatility\n",
            "(c)musk\n",
            "(d)tesla\n",
            "Correct answer is : (b)\n",
            "\n",
            "\n",
            "What does elon musk have about bitcoin?\n",
            "(a)elon musk\n",
            "(b)bitcoin\n",
            "(c)musk\n",
            "(d)intentions\n",
            "Correct answer is : (d)\n",
            "\n",
            "\n",
            "What type of organizations does Musk work for?\n",
            "(a)elon musk\n",
            "(b)bitcoin\n",
            "(c)musk\n",
            "(d)governments\n",
            "Correct answer is : (d)\n",
            "\n",
            "\n",
            "What does the narrative explore about elon musk and bitcoin?\n",
            "(a)elon musk\n",
            "(b)comments\n",
            "(c)musk\n",
            "(d)tesla\n",
            "Correct answer is : (b)\n",
            "\n",
            "\n",
            "What type of vehicle is elon musk known for?\n",
            "(a)electric vehicles\n",
            "(b)bitcoin\n",
            "(c)musk\n",
            "(d)tesla\n",
            "Correct answer is : (a)\n",
            "\n",
            "\n",
            "What does elon musk focus on?\n",
            "(a)energy consumption\n",
            "(b)bitcoin\n",
            "(c)musk\n",
            "(d)tesla\n",
            "Correct answer is : (a)\n",
            "\n",
            "\n",
            "What did critics of elon musk say about bitcoin?\n",
            "(a)elon musk\n",
            "(b)bitcoin\n",
            "(c)musk\n",
            "(d)critics argued\n",
            "Correct answer is : (d)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text_2 = \"Elon Musk and Bitcoin: A Complex Relationship.The intersection of Elon Musk, the enigmatic billionaire entrepreneur, and Bitcoin, the groundbreaking cryptocurrency, has been the subject of much fascination, speculation, and scrutiny in recent years. Musk, known for his ventures like Tesla, SpaceX, Neuralink, and The Boring Company, has proven to be a polarizing figure in the world of finance and technology, with his tweets and actions having significant implications for the price and perception of Bitcoin. This narrative explores the multifaceted relationship between Elon Musk and Bitcoin, delving into key events, controversies, and the broader implications for the cryptocurrency landscape.Elon Musk: A Brief Overview.Before delving into Musk's connection with Bitcoin, it's essential to understand who Elon Musk is and his role in the tech and automotive industries. Born in South Africa in 1971, Musk displayed a prodigious talent for technology from a young age. He moved to the United States to attend the University of Pennsylvania, where he earned dual bachelor's degrees in physics and economics.Musk's entrepreneurial journey began with the creation of Zip2, a software company he co-founded in 1995, which provided business directories and maps for newspapers. In 1999, Compaq acquired Zip2 for nearly $300 million, providing Musk with his first significant windfall.With his newfound wealth, Musk co-founded X.com, an online payment company, in 1999. X.com later evolved into PayPal and was sold to eBay in 2002 for $1.5 billion. Musk, however, did not rest on his laurels. Instead, he turned his attention to two ambitious and groundbreaking industries: electric vehicles and space exploration.Tesla, SpaceX, and Beyond.In 2004, Musk founded Tesla Motors (now Tesla, Inc.), with the goal of accelerating the world's transition to sustainable energy. Tesla's electric vehicles, beginning with the Roadster and followed by models like the Model S, Model 3, Model X, and Model Y, have transformed the automotive industry. Tesla's innovations in battery technology and electric drivetrains have played a crucial role in popularizing electric cars.Musk's SpaceX, founded in 2002, has achieved remarkable milestones in the field of space exploration. SpaceX developed the Falcon 1, the first privately developed liquid-fueled rocket to reach orbit, and later the Falcon 9 and Falcon Heavy, which have significantly reduced the cost of launching payloads into space. SpaceX also aims to establish a human presence on Mars through its Starship spacecraft.Beyond Tesla and SpaceX, Musk has pursued other ventures. Neuralink focuses on developing brain-computer interface technology, while The Boring Company is dedicated to creating underground transportation tunnels to alleviate urban congestion.Musk's achievements and ambitions have made him one of the most influential and scrutinized figures in the tech and business worlds. His social media presence, especially on Twitter, where he often shares updates on his companies and personal thoughts, has further amplified his reach and impact.Bitcoin: A Digital Revolution.Bitcoin, conceived in a 2008 whitepaper by the pseudonymous Satoshi Nakamoto, is a decentralized digital currency that operates on a peer-to-peer network. It represents a fundamental departure from traditional financial systems, as it doesn't rely on centralized authorities like banks or governments to validate and record transactions. Instead, Bitcoin transactions are confirmed by a network of computers, secured by cryptography, and recorded on a public ledger called the blockchain.Bitcoin's value proposition includes security, transparency, and the potential to serve as a store of value, similar to gold. It has gained traction as a means of transferring funds across borders, as a hedge against inflation, and as a speculative investment.As Bitcoin's popularity grew, it attracted attention from a diverse range of individuals and institutions, including tech entrepreneurs like Elon Musk.Elon Musk's Early Interest in Bitcoin.Musk's interest in Bitcoin became apparent through his tweets and comments on various occasions. While he didn't dive headfirst into the cryptocurrency, he showed a fascination with its technology and potential.In 2019, during a podcast interview with Ark Invest's Cathie Wood, Musk stated that Bitcoin is \\\"a far better way to transfer value than pieces of paper.\\\" He acknowledged its utility as a means of circumventing traditional financial intermediaries, highlighting its appeal to those who want greater control over their funds.Musk's acknowledgment of Bitcoin as a valuable innovation added to its credibility and mainstream acceptance. However, it was in the subsequent years that Musk's relationship with Bitcoin would become more complex, leading to significant fluctuations in the cryptocurrency's price and public perception.Tesla's Bitcoin Investment: A Game-Changer.The most pivotal moment in Elon Musk's involvement with Bitcoin came in early 2021 when Tesla, Inc. announced a groundbreaking move. In a filing with the U.S. Securities and Exchange Commission (SEC), Tesla revealed that it had purchased $1.5 billion worth of Bitcoin and intended to accept Bitcoin as a form of payment for its electric vehicles.This announcement sent shockwaves through the financial world, as it marked one of the most substantial endorsements of Bitcoin by a major corporation to date. Tesla's decision to allocate a portion of its corporate treasury to Bitcoin was seen as a bold move, signaling confidence in the cryptocurrency's long-term value.The rationale behind Tesla's Bitcoin investment, as outlined in its SEC filing, revolved around diversifying and maximizing returns on its cash reserves. Traditional avenues for cash management, like holding government bonds or placing funds in interest-bearing accounts, were yielding minimal returns. By allocating some of its capital to Bitcoin, Tesla sought to benefit from the cryptocurrency's potential for appreciation.This move had immediate repercussions on the price of Bitcoin, which surged to new all-time highs, surpassing $60,000 per BTC in early 2021. It also ignited a broader conversation about whether more corporations would follow Tesla's lead and allocate some of their treasuries to Bitcoin.Elon Musk's Tweets and Bitcoin's Volatility.Elon Musk's impact on Bitcoin extended beyond Tesla's investment. His prolific and often whimsical tweeting habits started influencing the cryptocurrency's price in ways that few could have predicted.Musk's tweets and public statements about Bitcoin and other cryptocurrencies were a double-edged sword. On one hand, his endorsement and public discussion of Bitcoin brought mainstream attention to the cryptocurrency, fueling interest and investment. On the other hand, his tweets could also lead to massive price swings, causing concern and frustration among investors and regulators.One of the most notable episodes occurred in May 2021 when Musk announced via Twitter that Tesla would no longer accept Bitcoin as payment for its vehicles, citing environmental concerns related to Bitcoin mining's energy consumption. This single tweet triggered a sharp decline in the price of Bitcoin, erasing billions of dollars in market value in a matter of hours.The environmental debate surrounding Bitcoin's energy use intensified following Musk's tweet. Critics argued that Bitcoin's energy consumption, driven by the proof-of-work consensus mechanism, was unsustainable and environmentally damaging. Proponents of Bitcoin countered that its energy use was comparable to, or even more efficient than, traditional banking and gold mining.Musk's influence over Bitcoin's price volatility didn't stop there. He continued to tweet about cryptocurrencies, often in cryptic and playful ways. For example, he tweeted about \\\"Baby Doge Coin\\\" and \\\"Shiba Inu,\\\" two meme cryptocurrencies, which led to speculative buying frenzies.These episodes prompted calls for greater regulation of Musk's Twitter activity and its potential impact on financial markets. Regulators expressed concerns about market manipulation and the need for greater transparency in Musk's communication regarding Tesla's Bitcoin holdings and intentions.Elon Musk's Impact on Altcoins.In addition to his influence on Bitcoin, Musk's tweets and comments have also affected various altcoins, which are cryptocurrencies other than Bitcoin. Dogecoin, in particular, emerged as a notable example of Musk's ability to move markets.Dogecoin, originally created as a meme cryptocurrency in 2013, gained a cult following on the internet due to its Shiba Inu dog logo and lighthearted community. However, its value remained relatively low and stable for years.In early 2021, Musk began tweeting about Dogecoin, referring to it as\\\"the people's crypto.\\\" His tweets, often accompanied by playful memes and comments, caused massive surges in Dogecoin\\'s price. At one point, it reached an all-time high of over $0.60 per DOGE, a substantial increase from its previous fractions-of-a-penny valuation.Musk's involvement with Dogecoin extended to his hosting of \\\"Saturday Night Live\\\" in May 2021. During the show, he referred to Dogecoin as a \\\"hustle,\\\" which led to a temporary price drop. Nevertheless, his overall engagement with Dogecoin contributed to its growing popularity and market capitalization.The phenomenon of \\\"Musk tweets\\\" driving the prices of cryptocurrencies sparked debates about the role of celebrity endorsements and social media influencers in the cryptocurrency space. It also raised questions about the fundamental value of assets like Dogecoin and the risks of investing based on internet trends and social media hype.The Impact of Elon Musk\\'s Environmental Concerns.Musk\\'s environmental concerns regarding Bitcoin mining were not limited to his May 2021 tweet. He continued to advocate for more sustainable practices within the cryptocurrency industry. He called on Bitcoin miners to transition to renewable energy sources and even engaged in discussions with prominent figures in the cryptocurrency community about potential solutions.These concerns about Bitcoin\\'s carbon footprint drew attention to a longstanding issue within the cryptocurrency space. Bitcoin's proof-of-work consensus mechanism requires significant computational power, which, in turn, demands substantial energy consumption. The majority of Bitcoin mining operations relied on fossil fuels, particularly coal, which contributed to concerns about its environmental impact.In response to Musk\\'s comments and mounting environmental criticism, some Bitcoin miners began exploring cleaner energy sources and sustainable practices. Several Bitcoin mining companies announced commitments to using renewable energy, and discussions about transitioning to a more eco-friendly consensus mechanism gained momentum.Musk's advocacy for sustainability in the cryptocurrency sector highlighted the growing importance of environmental, social, and governance (ESG) considerations for investors and corporations. It also led to broader conversations about the environmental impact of blockchain technologies and the need for responsible innovation in the crypto space.Elon Musk\\'s Evolving Stance on Bitcoin.Musk\\'s relationship with Bitcoin and his stance on the cryptocurrency have evolved over time. While he initially praised Bitcoin for its technological advantages and supported its adoption within Tesla, he later raised concerns about its environmental impact and price volatility.In July 2021, during a B Word conference, Musk revealed that SpaceX held Bitcoin and that he personally held Bitcoin, Ethereum, and Dogecoin. This disclosure indicated that, despite his reservations, Musk maintained a personal interest in cryptocurrencies.As the year progressed, Musk\\'s tweets about Bitcoin became less frequent, and his public statements about the cryptocurrency became more balanced. He acknowledged that he wanted to see Bitcoin succeed and that Tesla would likely accept Bitcoin again once its mining operations became more environmentally friendly.By late 2021, Musk indicated that he was working with Dogecoin developers to improve the cryptocurrency's efficiency, signaling his continued involvement in the crypto space.Elon Musk, Bitcoin, and Regulatory Scrutiny.The volatility in Bitcoin\\'s price caused by Musk\\'s tweets and statements raised concerns among regulators and lawmakers. The potential for market manipulation and the need for investor protection came into sharp focus.The U.S. Securities and Exchange Commission (SEC) and other regulatory bodies began examining Musk\\'s social media activity and its impact on cryptocurrency and stock markets. The SEC had previously clashed with Musk over his tweets regarding Tesla's stock, resulting in legal settlements and restrictions on his communication.While Musk\\'s influence over cryptocurrency markets is undoubtedly significant, the regulatory landscape for cryptocurrencies remains relatively nascent and complex. Regulators are grappling with how to address the unique challenges posed by the digital asset space, including the influence of high-profile individuals like Musk.Elon Musk's Vision for the Future of Cryptocurrency.Despite the complexities and controversies surrounding Elon Musk\\'s involvement with cryptocurrency, his vision for the future of digital assets and blockchain technology remains of interest.Musk has expressed support for the concept of decentralized finance (DeFi), which leverages blockchain technology to create open and permissionless financial systems. DeFi platforms enable activities like lending, borrowing, trading, and earning interest without the need for traditional financial intermediaries.Furthermore, Musk's involvement with SpaceX and his aspirations for Mars colonization have raised questions about the role of cryptocurrencies in space exploration. Some have speculated that cryptocurrencies could become the primary means of conducting financial transactions and economic activities on future Martian colonies.Additionally, Musk\\'s advocacy for renewable energy solutions aligns with the broader trend of sustainable blockchain technologies. Several cryptocurrencies, like Ethereum, are transitioning from proof-of-work to proof-of-stake consensus mechanisms, which are more energy-efficient and environmentally friendly. Musk's influence and financial resources could potentially contribute to the development and adoption of greener blockchain solutions.Conclusion: The Ongoing Saga of Elon Musk and Bitcoin.The relationship between Elon Musk and Bitcoin has been a rollercoaster ride, characterized by enthusiasm, controversy, and unpredictability. Musk\\'s tweets have demonstrated the power of celebrity influence in the world of cryptocurrency, shaping market sentiment and investor behavior.While Musk\\'s statements and actions have generated significant volatility in cryptocurrency markets, they have also drawn attention to important issues within the industry, such as environmental sustainability and responsible innovation. His involvement has forced a broader discussion about the role of cryptocurrencies in the global economy and their potential to reshape traditional financial systems.As cryptocurrency continues to evolve and mature, it remains to be seen how Musk\\'s influence and vision will intersect with this rapidly changing landscape. Whether through his investments, innovations, or advocacy for sustainability, Elon Musk will likely continue to be a central figure in the ongoing narrative of cryptocurrencies and their place in the digital age.\"\n",
        "\n",
        "final_questions = get_mca_questions(text_2)\n",
        "for q in final_questions:\n",
        "    print(q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "OsI6-UEerYbc",
        "outputId": "f1fa0018-116d-4d44-93f6-bd37fc890382"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyngrok'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fbc0ed51abc8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Replace <YOUR_AUTHTOKEN> with your actual token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2qWVnkLoqgqfkvur2I5GanFqiRg_7ZEVkerG2xDb9msUXTnk3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyngrok'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace <YOUR_AUTHTOKEN> with your actual token\n",
        "ngrok.set_auth_token(\"2qWVnkLoqgqfkvur2I5GanFqiRg_7ZEVkerG2xDb9msUXTnk3\")\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"Public URL:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty5pagTlqDDT",
        "outputId": "3d1cd4c3-c11e-458e-e8fb-f20eabaa26ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.2)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Public URL: NgrokTunnel: \"https://5491-34-86-229-26.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Dec/2024 12:15:33] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "!pip install flask pyngrok\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import random\n",
        "\n",
        "# Flask app setup\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# Assuming you have these functions available in your environment:\n",
        "# summarizer, get_keywords, get_question, get_distractors, etc.\n",
        "\n",
        "def get_mca_questions(text, num_questions=5):\n",
        "    \"\"\"\n",
        "    This function generates multiple-choice questions based on a given context.\n",
        "    It summarizes the context, extracts important keywords, generates questions related to those keywords,\n",
        "    and provides randomized answer choices, including the correct answer, for each question.\n",
        "    The `num_questions` parameter controls how many questions to generate.\n",
        "    \"\"\"\n",
        "    # Summarize the context text (you need a summarization model for this)\n",
        "    summarized_text = summarizer(text, summary_model, summary_tokenizer)\n",
        "\n",
        "    # Extract keywords or important phrases from the summarized text\n",
        "    imp_keywords = get_keywords(summarized_text)\n",
        "\n",
        "    output_list = []\n",
        "\n",
        "    for keyword in imp_keywords[:num_questions]:  # Limit to `num_questions`\n",
        "        output = \"\"\n",
        "\n",
        "        # Generate a question based on the keyword and the summarized text\n",
        "        ques = get_question(summarized_text, keyword, question_model, question_tokenizer)\n",
        "\n",
        "        # Generate distractors for the question\n",
        "        distractors = get_distractors(keyword.capitalize(), ques, s2v, sentence_transformer_model, 40, 0.2)\n",
        "\n",
        "        output += ques + \"\\n\"  # Add the question text\n",
        "\n",
        "        if len(distractors) == 0:\n",
        "            distractors = imp_keywords  # Use the keywords themselves if no distractors are found\n",
        "\n",
        "        # Randomly choose where to place the correct answer\n",
        "        random_integer = random.randint(0, 3)  # Randomize answer order\n",
        "        alpha_list = ['(a)', '(b)', '(c)', '(d)']\n",
        "\n",
        "        # Add the options (correct answer and distractors)\n",
        "        for d, distractor in enumerate(distractors[:4]):\n",
        "            if d == random_integer:\n",
        "                output += alpha_list[d] + keyword + \"\\n\"  # Correct answer in random position\n",
        "            else:\n",
        "                output += alpha_list[d] + distractor + \"\\n\"  # Distractor options\n",
        "\n",
        "        # Add the correct answer to the output\n",
        "        output += \"Correct answer is: \" + alpha_list[random_integer] + \"\\n\\n\"\n",
        "\n",
        "        output_list.append(output)  # Add this question block to the output list\n",
        "\n",
        "    return output_list\n",
        "\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate_mcqs():\n",
        "    data = request.json\n",
        "    if not data or 'text' not in data:\n",
        "        return jsonify({\"error\": \"Invalid input\"}), 400\n",
        "\n",
        "    text = data['text']\n",
        "    questions = get_mca_questions(text)\n",
        "    return jsonify({\"questions\": questions})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2AlFafYBwiqn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}